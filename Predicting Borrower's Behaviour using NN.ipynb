{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting whether a loan borrower will pay back or not using Neural Networks.\n",
    "\n",
    "## The Data\n",
    "\n",
    "We will be using a subset of the LendingClub DataSet obtained from Kaggle: https://www.kaggle.com/wordsforthewise/lending-club\n",
    "\n",
    "\n",
    "LendingClub is a US peer-to-peer lending company, headquartered in San Francisco, California.[3] It was the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission (SEC), and to offer loan trading on a secondary market. LendingClub is the world's largest peer-to-peer lending platform.\n",
    "\n",
    "### Our Goal\n",
    "\n",
    "Given historical data on loans given out with information on whether or not the borrower defaulted (charge-off), can we build a model thatcan predict wether or nor a borrower will pay back their loan? This way in the future when we get a new potential customer we can assess whether or not they are likely to pay back the loan. Keep in mind classification metrics when evaluating the performance of your model!\n",
    "\n",
    "The \"loan_status\" column contains our label.\n",
    "\n",
    "### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# might be needed depending on your version of Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is already pre-processed with feature engineering & Explatory Data Analysis\n",
    "import pickle\n",
    "with open('my.pickle', 'rb') as data:\n",
    "    df1=pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Scikit learn( fit the data to train) & use MinMaxScaler for normalising data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the X to be everything expect loan_repaid as loan_repaid is what we want to predict\n",
    "X = df1.drop('loan_repaid',axis=1).values\n",
    "# y is our predictor so we want to predict 'loan_repaid'\n",
    "y = df1['loan_repaid'].values\n",
    "#fit the train test into x and y with 80% training and 20% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data and avoid testing data leakage \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler= MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Quick Note on our Neural Network Build </h2>\n",
    "<br>Building a NN( sequential model)\n",
    "<br>We have different dense units: 78 dense units --> 39 dense unit --> 19 densue unit--> 1 output neuron. \n",
    "<br>We use dropouts to prevent overfitting and turn off neurons if they do. \n",
    "<br> We use rectified Linear Unit activation for most part and sigmoid activation for last output neurons.\n",
    "<br>Using sigmoid activation for the last output neuron as this is binary classification problem. \n",
    "<br>Using adam optimiser with binary crossentrophy loss measurement. We then visualise these losses.\n",
    "<br> EarlyStopping mechanism when training so it stops at perfect epochs(iterations)\n",
    "<br> Optional: Using Tensorboard but it won't be as useful as this is not Convoluted Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "9881/9881 [==============================] - 45s 5ms/step - loss: 0.2982 - val_loss: 0.2653\n",
      "Epoch 2/600\n",
      "9881/9881 [==============================] - 40s 4ms/step - loss: 0.2705 - val_loss: 0.2646\n",
      "Epoch 3/600\n",
      "9881/9881 [==============================] - 42s 4ms/step - loss: 0.2693 - val_loss: 0.2654\n",
      "Epoch 4/600\n",
      "9881/9881 [==============================] - 41s 4ms/step - loss: 0.2686 - val_loss: 0.2657\n",
      "Epoch 5/600\n",
      "9881/9881 [==============================] - 49s 5ms/step - loss: 0.2683 - val_loss: 0.2636\n",
      "Epoch 6/600\n",
      "9881/9881 [==============================] - 41s 4ms/step - loss: 0.2685 - val_loss: 0.2651\n",
      "Epoch 7/600\n",
      "9881/9881 [==============================] - 42s 4ms/step - loss: 0.2675 - val_loss: 0.2641\n",
      "Epoch 8/600\n",
      "9881/9881 [==============================] - 40s 4ms/step - loss: 0.2674 - val_loss: 0.2634\n",
      "Epoch 9/600\n",
      "9881/9881 [==============================] - 47s 5ms/step - loss: 0.2676 - val_loss: 0.2638\n",
      "Epoch 10/600\n",
      "9881/9881 [==============================] - 42s 4ms/step - loss: 0.2670 - val_loss: 0.2639\n",
      "Epoch 11/600\n",
      "9881/9881 [==============================] - 45s 5ms/step - loss: 0.2671 - val_loss: 0.2642\n",
      "Epoch 12/600\n",
      "9881/9881 [==============================] - 45s 5ms/step - loss: 0.2672 - val_loss: 0.2652\n",
      "Epoch 13/600\n",
      "9881/9881 [==============================] - 47s 5ms/step - loss: 0.2670 - val_loss: 0.2644\n",
      "Epoch 14/600\n",
      "9881/9881 [==============================] - 49s 5ms/step - loss: 0.2671 - val_loss: 0.2638\n",
      "Epoch 15/600\n",
      "9881/9881 [==============================] - 44s 4ms/step - loss: 0.2666 - val_loss: 0.2645\n",
      "Epoch 16/600\n",
      "9881/9881 [==============================] - 41s 4ms/step - loss: 0.2669 - val_loss: 0.2641\n",
      "Epoch 17/600\n",
      "9881/9881 [==============================] - 59s 6ms/step - loss: 0.2670 - val_loss: 0.2637\n",
      "Epoch 18/600\n",
      "9881/9881 [==============================] - 38s 4ms/step - loss: 0.2668 - val_loss: 0.2643\n",
      "Epoch 19/600\n",
      "9881/9881 [==============================] - 30s 3ms/step - loss: 0.2673 - val_loss: 0.2652\n",
      "Epoch 20/600\n",
      "9881/9881 [==============================] - 37s 4ms/step - loss: 0.2669 - val_loss: 0.2635\n",
      "Epoch 21/600\n",
      "9881/9881 [==============================] - 47s 5ms/step - loss: 0.2663 - val_loss: 0.2643\n",
      "Epoch 22/600\n",
      "9881/9881 [==============================] - 43s 4ms/step - loss: 0.2668 - val_loss: 0.2646\n",
      "Epoch 23/600\n",
      "9881/9881 [==============================] - 43s 4ms/step - loss: 0.2669 - val_loss: 0.2638\n",
      "Epoch 24/600\n",
      "9881/9881 [==============================] - 44s 4ms/step - loss: 0.2668 - val_loss: 0.2637\n",
      "Epoch 25/600\n",
      "9881/9881 [==============================] - 48s 5ms/step - loss: 0.2668 - val_loss: 0.2637\n",
      "Epoch 26/600\n",
      "9881/9881 [==============================] - 47s 5ms/step - loss: 0.2671 - val_loss: 0.2646\n",
      "Epoch 27/600\n",
      "9881/9881 [==============================] - 58s 6ms/step - loss: 0.2666 - val_loss: 0.2632\n",
      "Epoch 28/600\n",
      "9881/9881 [==============================] - 47s 5ms/step - loss: 0.2667 - val_loss: 0.2646\n",
      "Epoch 29/600\n",
      "9881/9881 [==============================] - 46s 5ms/step - loss: 0.2664 - val_loss: 0.2652\n",
      "Epoch 30/600\n",
      "9881/9881 [==============================] - 51s 5ms/step - loss: 0.2666 - val_loss: 0.2642\n",
      "Epoch 31/600\n",
      "9881/9881 [==============================] - 37s 4ms/step - loss: 0.2667 - val_loss: 0.2645\n",
      "Epoch 32/600\n",
      "9881/9881 [==============================] - 36s 4ms/step - loss: 0.2672 - val_loss: 0.2644\n",
      "Epoch 33/600\n",
      "9881/9881 [==============================] - 41s 4ms/step - loss: 0.2667 - val_loss: 0.2648\n",
      "Epoch 34/600\n",
      "9881/9881 [==============================] - 56s 6ms/step - loss: 0.2668 - val_loss: 0.2646\n",
      "Epoch 35/600\n",
      "9881/9881 [==============================] - 45s 5ms/step - loss: 0.2666 - val_loss: 0.2644\n",
      "Epoch 36/600\n",
      "9881/9881 [==============================] - 50s 5ms/step - loss: 0.2674 - val_loss: 0.2663\n",
      "Epoch 37/600\n",
      "9881/9881 [==============================] - 46s 5ms/step - loss: 0.2668 - val_loss: 0.2656\n",
      "Epoch 38/600\n",
      "9881/9881 [==============================] - 35s 4ms/step - loss: 0.2672 - val_loss: 0.2649\n",
      "Epoch 39/600\n",
      "9881/9881 [==============================] - 48s 5ms/step - loss: 0.2662 - val_loss: 0.2653\n",
      "Epoch 40/600\n",
      "9881/9881 [==============================] - 43s 4ms/step - loss: 0.2665 - val_loss: 0.2662\n",
      "Epoch 41/600\n",
      "9881/9881 [==============================] - 54s 5ms/step - loss: 0.2668 - val_loss: 0.2663\n",
      "Epoch 42/600\n",
      "9881/9881 [==============================] - 46s 5ms/step - loss: 0.2670 - val_loss: 0.2657\n",
      "Epoch 43/600\n",
      "9881/9881 [==============================] - 48s 5ms/step - loss: 0.2665 - val_loss: 0.2652\n",
      "Epoch 44/600\n",
      "9881/9881 [==============================] - 48s 5ms/step - loss: 0.2666 - val_loss: 0.2663\n",
      "Epoch 45/600\n",
      "9881/9881 [==============================] - 59s 6ms/step - loss: 0.2666 - val_loss: 0.2668\n",
      "Epoch 46/600\n",
      "9881/9881 [==============================] - 48s 5ms/step - loss: 0.2667 - val_loss: 0.2659\n",
      "Epoch 47/600\n",
      "9881/9881 [==============================] - 46s 5ms/step - loss: 0.2664 - val_loss: 0.2659\n",
      "Epoch 48/600\n",
      "9881/9881 [==============================] - 29s 3ms/step - loss: 0.2662 - val_loss: 0.2684\n",
      "Epoch 49/600\n",
      "9881/9881 [==============================] - 45s 5ms/step - loss: 0.2660 - val_loss: 0.2663\n",
      "Epoch 50/600\n",
      "9881/9881 [==============================] - 49s 5ms/step - loss: 0.2662 - val_loss: 0.2669\n",
      "Epoch 51/600\n",
      "9881/9881 [==============================] - 45s 5ms/step - loss: 0.2666 - val_loss: 0.2661\n",
      "Epoch 52/600\n",
      "9881/9881 [==============================] - 43s 4ms/step - loss: 0.2666 - val_loss: 0.2670\n",
      "Epoch 00052: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18d2b169508>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#check out shape of train data so we know how many Neuron unit we need\n",
    "X_train.shape # we have 426 elements with 30 features so 30 units of NN\n",
    "model=Sequential()\n",
    "model.add(Dense(78, activation='relu'))# put 78 NN\n",
    "model.add(Dropout(0.5))# 0- no neurons turnt off, 1 = 100% neurons turnt off, use 0.2-0.5\n",
    "model.add(Dense(39, activation='relu'))# cut in half almost\n",
    "model.add(Dropout(0.5))# 0- no neurons turnt off, 1 = 100% neurons turnt off, use 0.2-0.5\n",
    "model.add(Dense(19, activation='relu'))# cut in half again\n",
    "model.add(Dropout(0.5))# 0- no neurons turnt off, 1 = 100% neurons turnt off, use 0.2-0.5\n",
    "# BINARY CLASSIFICATION AS IN LOAN-PAID=1 AND LOAN-NOT-PAID=0 SO USE SIGMOID ACTIVATION\n",
    "model.add(Dense(1, activation='sigmoid'))# we want 1 NN output with sigmoid activation\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "# early stopping mechanism to prevent overfitting\n",
    "early_stop= EarlyStopping(monitor='val_loss', mode='min' , verbose=1, patience=25)# have patience after 25iterations\n",
    "# mode what is what your trying to(min for loss or max for accuracy)\n",
    "# verbose gives report\n",
    "# train the model using fit\n",
    "model.fit(x=X_train, y=y_train, epochs=600,validation_data=(X_test,y_test),\n",
    "         callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the model so we can instantly check new customers without training NN model again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-1da3bd04dde6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#by loading this .h5 files on it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loan_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mlater_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loan_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#by saving the command above, we can instantly check new price of any customers loan_status\n",
    "#by loading this .h5 files on it \n",
    "from tensorflow.keras.models import load_model\n",
    "model.save('loan_model.h5')\n",
    "later_model=load_model('loan_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluating Model Performance.\n",
    "\n",
    "**Plotting out the validation loss versus the training loss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss= pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK: Create predictions from the X_test set and display a classification report and confusion matrix for the X_test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-186-5dcfb0dffdca>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.43      0.60     15658\n",
      "           1       0.88      1.00      0.93     63386\n",
      "\n",
      "    accuracy                           0.89     79044\n",
      "   macro avg       0.94      0.72      0.77     79044\n",
      "weighted avg       0.90      0.89      0.87     79044\n",
      "\n",
      "[[ 6739  8919]\n",
      " [   14 63372]]\n"
     ]
    }
   ],
   "source": [
    "#lets predict & evaluate with keras\n",
    "predictions= model.predict_classes(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions)) # they were not perfectly balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given the customer below, would you offer this person a loan?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                                    24000\n",
       "term                                     60 months\n",
       "int_rate                                     13.11\n",
       "installment                                 547.43\n",
       "grade                                            B\n",
       "sub_grade                                       B4\n",
       "emp_length                               10+ years\n",
       "home_ownership                            MORTGAGE\n",
       "annual_inc                                   85000\n",
       "verification_status                Source Verified\n",
       "issue_d                                   Jan-2013\n",
       "loan_status                             Fully Paid\n",
       "purpose                                credit_card\n",
       "title                                         Debt\n",
       "dti                                          10.98\n",
       "earliest_cr_line                          Oct-1991\n",
       "open_acc                                         6\n",
       "pub_rec                                          0\n",
       "revol_bal                                    35464\n",
       "revol_util                                    66.8\n",
       "total_acc                                       29\n",
       "initial_list_status                              f\n",
       "application_type                        INDIVIDUAL\n",
       "mort_acc                                         8\n",
       "pub_rec_bankruptcies                             0\n",
       "address                 USS Sheppard\\nFPO AP 05113\n",
       "Name: 304691, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so i picked a new customer and i want to know what the loan_status is  \n",
    "import random\n",
    "random.seed(101)\n",
    "random_ind= random.randint(0,len(df))\n",
    "new_person=df.drop('loan_repaid', axis=1).iloc[random_ind]\n",
    "new_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[random_ind]['loan_repaid']\n",
    "# 1 means paid, 0 means default(they did not pay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
